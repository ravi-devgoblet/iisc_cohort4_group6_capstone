{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaKaDC34WeSa0rwKpYbK4j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"NigEMu1rf_Zz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://huggingface.co/edugp/kenlm/resolve/main/wikipedia/en.arpa.bin?download=true"],"metadata":{"id":"uRzlupXBpbZ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install python_speech_features"],"metadata":{"id":"ZuQ057l6B0Pm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import librosa\n","from python_speech_features import mfcc\n","from librosa.feature import spectral_centroid, chroma_stft\n","from sklearn.preprocessing import LabelEncoder\n","import kenlm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n","from keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"iT2kXdcTktZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_features(audio_file, num_mfcc=13, n_fft=2048):\n","    audio, sr = librosa.load(audio_file, sr=None)\n","\n","    # Extract MFCC features\n","    mfcc_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=num_mfcc, n_fft=n_fft)\n","    #print(\"MFCC features shape:\", mfcc_features.shape)\n","\n","    # Extract spectral centroid\n","    #spectral_centroid_features = librosa.feature.spectral_centroid(y=audio, sr=sr)\n","\n","    # Extract chroma features\n","    #chroma_features = librosa.feature.chroma_stft(y=audio, sr=sr)\n","\n","    # Ensure all features have the same length\n","    max_length = 317\n","    mfcc_features = pad_or_truncate(mfcc_features, max_length)\n","    #spectral_centroid_features = pad_or_truncate(spectral_centroid_features, max_length)\n","    #chroma_features = pad_or_truncate(chroma_features, max_length)\n","\n","    # Concatenate features\n","#    features = np.concatenate([mfcc_features, spectral_centroid_features, chroma_features], axis=0)\n","\n","    return mfcc_features\n","\n","def pad_or_truncate(array, target_length):\n","    current_length = array.shape[1]\n","    if current_length < target_length:\n","        # Pad the array\n","        padding = target_length - current_length\n","        pad_width = ((0, 0), (0, padding))\n","        padded_array = np.pad(array, pad_width=pad_width, mode='constant', constant_values=0)\n","        return padded_array\n","    elif current_length > target_length:\n","        # Truncate the array\n","        truncated_array = array[:, :target_length]\n","        return truncated_array\n","    else:\n","        return array\n","\n","# Function to handle OOV words\n","def handle_oov_words(transcript, language_model):\n","    tokens = transcript.split()\n","    corrected_transcript = []\n","\n","    for token in tokens:\n","        if not language_model.score(token):  # Check if token is OOV\n","            # Get candidate words from language model\n","            candidates = generate_candidates(token, language_model)\n","\n","            # Choose the most likely candidate\n","            corrected_token = max(candidates, key=lambda x: x[1])[0]\n","            corrected_transcript.append(corrected_token)\n","        else:\n","            corrected_transcript.append(token)\n","\n","    return ' '.join(corrected_transcript)\n","\n","# Function to generate candidate corrections for OOV word\n","def generate_candidates(oov_word, language_model, num_candidates=5):\n","    candidates = []\n","\n","    # Score candidate words based on the language model\n","    for word in language_model:\n","        score = language_model.score(oov_word + ' ' + word)\n","        candidates.append((word, score))\n","\n","    # Sort candidates by score\n","    candidates.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Return top num_candidates candidates\n","    return candidates[:num_candidates]\n"],"metadata":{"id":"UxMX10r5kwU6","executionInfo":{"status":"ok","timestamp":1713091641347,"user_tz":-330,"elapsed":424,"user":{"displayName":"Ravi Kappagantu","userId":"16635277517994618813"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# Directory containing audio files and transcripts\n","data_dir = '/content/drive/MyDrive/Capstone/TestFiles/'\n","\n","# Load transcripts\n","transcripts = []\n","for filename in os.listdir(data_dir):\n","    if filename.endswith('.txt'):\n","        with open(os.path.join(data_dir, filename), 'r') as f:\n","            transcripts.append({'audio_file': filename.replace('.txt', '.wav'), 'transcript': f.read()})\n","\n","transcripts_df = pd.DataFrame(transcripts)"],"metadata":{"id":"SORn0c1Kk3V6","executionInfo":{"status":"ok","timestamp":1713093090024,"user_tz":-330,"elapsed":1025,"user":{"displayName":"Ravi Kappagantu","userId":"16635277517994618813"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["# Load label encoder\n","label_encoder = LabelEncoder()\n","label_encoder.fit(transcripts_df['transcript'])\n","\n","# Load language model\n","language_model = kenlm.LanguageModel('/content/en.arpa.bin')"],"metadata":{"id":"cjZngZnUk76z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocess transcripts to handle OOV words\n","transcripts_df['processed_transcript'] = transcripts_df['transcript'].apply(lambda x: handle_oov_words(x, language_model))"],"metadata":{"id":"VJjszRDPlAAz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract features from audio files\n","max_length = 0\n","features = []\n","\n","for audio_file in transcripts_df['audio_file']:\n","    feature = extract_features(os.path.join(data_dir, audio_file))\n","    features.append(feature)\n","    max_length = max(max_length, feature.shape[1])\n","\n","print(max_length)\n","\n","X = np.array(features)\n","y = label_encoder.transform(transcripts_df['processed_transcript'])"],"metadata":{"id":"znOneSR8lB98","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713091656240,"user_tz":-330,"elapsed":5119,"user":{"displayName":"Ravi Kappagantu","userId":"16635277517994618813"}},"outputId":"cc54935f-31a7-461e-af90-8bba92044994"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["317\n"]}]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define and train the ASR model\n","model = Sequential([\n","    Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])),\n","    Bidirectional(LSTM(32)),\n","    Dense(len(label_encoder.classes_), activation='softmax')\n","])"],"metadata":{"id":"k-tJBw78lL8-","executionInfo":{"status":"ok","timestamp":1713094484980,"user_tz":-330,"elapsed":1737,"user":{"displayName":"Ravi Kappagantu","userId":"16635277517994618813"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"],"metadata":{"id":"RwBL_72elOpu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713094506771,"user_tz":-330,"elapsed":17189,"user":{"displayName":"Ravi Kappagantu","userId":"16635277517994618813"}},"outputId":"481fcf7f-5d7a-4e5a-ed16-4c466f0dc36a"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","4/4 [==============================] - 9s 668ms/step - loss: 2.2565 - accuracy: 0.2478 - val_loss: 2.0752 - val_accuracy: 0.3448\n","Epoch 2/10\n","4/4 [==============================] - 0s 108ms/step - loss: 1.7185 - accuracy: 0.8319 - val_loss: 1.8320 - val_accuracy: 0.7241\n","Epoch 3/10\n","4/4 [==============================] - 0s 108ms/step - loss: 1.3623 - accuracy: 0.9381 - val_loss: 1.6265 - val_accuracy: 0.7586\n","Epoch 4/10\n","4/4 [==============================] - 0s 111ms/step - loss: 1.0660 - accuracy: 0.9912 - val_loss: 1.4458 - val_accuracy: 0.7931\n","Epoch 5/10\n","4/4 [==============================] - 0s 99ms/step - loss: 0.8027 - accuracy: 1.0000 - val_loss: 1.2369 - val_accuracy: 0.8621\n","Epoch 6/10\n","4/4 [==============================] - 0s 108ms/step - loss: 0.5698 - accuracy: 1.0000 - val_loss: 1.0449 - val_accuracy: 0.8621\n","Epoch 7/10\n","4/4 [==============================] - 0s 104ms/step - loss: 0.3884 - accuracy: 1.0000 - val_loss: 0.8910 - val_accuracy: 0.8621\n","Epoch 8/10\n","4/4 [==============================] - 0s 103ms/step - loss: 0.2630 - accuracy: 1.0000 - val_loss: 0.7492 - val_accuracy: 0.8966\n","Epoch 9/10\n","4/4 [==============================] - 0s 83ms/step - loss: 0.1748 - accuracy: 1.0000 - val_loss: 0.6539 - val_accuracy: 0.9310\n","Epoch 10/10\n","4/4 [==============================] - 0s 107ms/step - loss: 0.1185 - accuracy: 1.0000 - val_loss: 0.5772 - val_accuracy: 0.8966\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7cda519c2440>"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["!pip install jiwer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VpAdHe8iPJPa","executionInfo":{"status":"ok","timestamp":1713091965124,"user_tz":-330,"elapsed":13086,"user":{"displayName":"Ravi Kappagantu","userId":"16635277517994618813"}},"outputId":"17e05c35-04c7-41f1-df7c-346c68526e2f"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting jiwer\n","  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n","Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n","Collecting rapidfuzz<4,>=3 (from jiwer)\n","  Downloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n","Successfully installed jiwer-3.0.3 rapidfuzz-3.8.1\n"]}]},{"cell_type":"code","execution_count":77,"metadata":{"id":"CP7gDzTQdY0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713094515491,"user_tz":-330,"elapsed":2698,"user":{"displayName":"Ravi Kappagantu","userId":"16635277517994618813"}},"outputId":"83cda15a-534c-4067-c627-97d37ce99db1"},"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 2s 12ms/step\n","Word Error Rate (WER): 0.0205761316872428\n","Accuracy: 0.9722222222222222\n"]}],"source":["from jiwer import wer\n","from sklearn.metrics import accuracy_score\n","\n","# Get raw predictions\n","y_pred_prob = model.predict(X_test)\n","\n","# Convert probabilities to class predictions\n","y_pred = np.argmax(y_pred_prob, axis=1)\n","\n","# Convert the integer-encoded predictions back to words using the label encoder\n","y_pred_words = label_encoder.inverse_transform(y_pred)\n","\n","# Convert the integer-encoded ground truth transcriptions back to words\n","y_test_words = label_encoder.inverse_transform(y_test)\n","\n","# Convert predictions and ground truth transcriptions to lists of strings\n","y_pred_words = list(map(str, y_pred_words))\n","y_test_words = list(map(str, y_test_words))\n","\n","# Calculate Word Error Rate (WER)\n","wer_score = wer(y_test_words, y_pred_words)\n","print(\"Word Error Rate (WER):\", wer_score)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)"]},{"cell_type":"code","source":["audio_file = \"/content/sample_data/Test Audio - K T - U S - 2.wav\"\n","\n","# Extract features from audio file\n","features = extract_features(audio_file)\n","\n","# Add batch dimension to the features\n","features = np.expand_dims(features, axis=0)\n","\n","# Get raw predictions from a file\n","y_pred_prob = model.predict(features)\n","\n","# Convert probabilities to class predictions\n","y_pred = np.argmax(y_pred_prob, axis=1)\n","\n","# Convert the integer-encoded predictions back to words using the label encoder\n","y_pred_words = label_encoder.inverse_transform(y_pred)\n","\n","print(y_pred_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGPc5e0JR_e5","executionInfo":{"status":"ok","timestamp":1713094707883,"user_tz":-330,"elapsed":436,"user":{"displayName":"Ravi Kappagantu","userId":"16635277517994618813"}},"outputId":"b7d84c54-8348-4660-8002-6ff77b25117d"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 70ms/step\n","['He is a quality player.']\n"]}]}]}